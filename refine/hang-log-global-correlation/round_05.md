# Round 5: 「実践でのハング検出 → Core自動自己修正フロー」

**日時**: 2026-02-24T11:11:00+09:00
**前ラウンドからの焦点**: 学習ループが閉じない問題（brain_log揮発・watchdog隠蔽・転記トリガー不在）
**今回の命題（ユーザー提起）**: エラーテストが捉えられない実践ハングこそが主要ボトルネック。ハング検出したら「報告→Core根本修正→相関箇所全クリーン」を自動実行すべき。

---

## 議論

**🤔 Skeptic**:
「"実践でのハング検出が精度高い" という前提を疑う。

ハングを検出できるのは、AIエージェント自身か、それともユーザーか。現在の仕組みは、AIがコマンドを打って30秒待って "No output" を確認する——これは `run_command` というターミナルツールの上でしか動いていない。

ではSSHフリーズは？ Notion APIのネットワーク詰まりは？ ブラウザSAの沈黙は？ これらはそれぞれ別のレイヤーで詰まり、30秒ルールではなく、10秒、60秒、∞秒とバラバラだ。

ハング検出の精度が高いと言えるのは、**"何をもってハングと定義するか" の統一基準が存在する場合だけだ**。今それは存在するか？ `safe-commands.md` には `30秒ルール` があるが、Notion API待ちは60秒でも正常かもしれない。SSHは10秒で既に異常だ。

つまり "実践で検出できる" と "正確に検出できる" は別の命題だ。」

---

**😈 Devil's Advocate**:
「ユーザーが言う "Core自体の修正" という発想の危険性を逆張りする。

Core（`~/.antigravity`）はすべてのプロジェクトが共有するWFエンジンだ。ハング発生のたびにAIがCoreを自動修正するということは、**実験的な修正が本番共有インフラに即座に適用される**ことを意味する。

修正が誤っていた場合——たとえばtimeout値を誤って1秒に設定した場合——次のセッションから全ての `_smart_run` が1秒で kill されるようになる。エラーの伝播が下流に全波及する。

ならば、"ハング報告→Coreの修正提案→ユーザー承認→適用" というフローにすべきではないか。完全自動修正より「提案 + 承認ゲート」の方が安全ではないか。

もしくは修正の適用先を **Core本体ではなくCore-B（`~/.gemini/antigravity`）の staging layer** にしてから、明示的にCore-Aに昇格させる段階的アーキテクチャなら両立できる。」

---

**🔬 Systems Analyst**:
「"相関関係を把握して関連する箇所を全てクリーン" という動作を実現するために必要なものを分解する。

現在Antigravityには **依存グラフが存在しない**。

`checkout.md` が `session_state.js` を呼び、`session_state.js` が `brain_log/` に書き、`brain_log/` が `checkin.md` に読まれる——このチェーンはドキュメントではなく、「読む人がいれば分かる」レベルで暗黙に存在している。

相関箇所をクリーンにするためには：
1. **WF間依存マップ**が必要（どのWFがどのスクリプト/どのファイルを読み書きするか）
2. ハングが `session_state.js` で発生した場合、それを呼ぶすべてのWF（checkout/checkin/evolve）が影響を受けると判定できる仕組み
3. "クリーン" の定義が必要（lock除去か / 設定リセットか / ファイル再生成か）

依存マップがなければ "相関箇所" を特定することは不可能だ。推測でcleanupを実行するのはむしろ危険。**まず依存グラフの明文化が前提条件になる。**」

---

**🧠 Root Cause Hunter**:
「"エラーテストだけでは捉えられない実践エラー" の正体を5 Whysで掘る。

**Why 1**: なぜテストが実践エラーを捉えられないのか
→ テストは「期待するシナリオ」を検証するが、ハングは「期待から外れた環境」で起きる（SSD未マウント、ネットワーク切断、プロセスロック残存）

**Why 2**: なぜそういった環境状態がテストで再現できないのか
→ mocha/vitest等のテストランナーは、OSのマウント状態やネットワーク状態を変化させながら実行することが通常できない。Chaos Engineeringの領域になる。

**Why 3**: なぜChaos Engineeringが導入されていないのか
→ Antigravityでは `chaos_monkey.js` というスクリプトが存在するが、checkout/checkinのパイプラインに組み込まれていない

**Why 4**: なぜ `chaos_monkey.js` が組み込まれていないのか
→ "実践でしか分からない問題" に対して "実践を意図的に壊すテスト" という発想が、設計時に連結されていなかった

**Why 5（根本）**: **実践エラー = 環境の非決定論的状態変化 = Chaos Engineering の問題領域。そのキーワードがAntigravityの設計語彙に入っていない。**

→ したがって「ハング検出→Core修正」のパイプラインに加えて、**「意図的にハング状態を誘発するChaos層」** を定期実行すれば、実践と同等の精度でエラーを継続的に検出できる。これが閉じた学習ループの完成形だ。」

---

## 🧭 Moderator Summary（Round 5）

| 項目 | 内容 |
|------|------|
| **明確になったこと** | ①ハング検出の定義が未統一（30秒ルールはターミナル限定）②Core自動修正は "承認ゲート" なしでは危険③相関箇所クリーンには依存グラフが前提条件④実践エラー = Chaos Engineering問題領域。`chaos_monkey.js` が存在するが孤立している |
| **新規概念** | **「Core staging layer」**（Core-B → 承認 → Core-A）/ **「Chaos層定期実行」**（実践エラーを意図的に再現する継続的テスト）|
| **ユーザー命題への判定** | ハング検出→報告→Core修正の方向性は正しい。ただし「自動修正」は段階的に: `検出→報告（実装済）→提案生成（未実装）→承認ゲート（未実装）→適用` が安全なフロー |
| **まだ詰まっていない論点** | 依存グラフをどう表現するか（JSON? Markdown?) / Chaos MonkeyをCoreパイプラインに組み込む適切なタイミングはいつか / "承認ゲート" は人間介在かAI自律かどちらが適切か |
| **判定** | `Continue` — 依存グラフ設計とChaos層統合という2つの具体的命題が出た。Round 6で詰めるか、または `Conclude` して `/gen-dev` へ |
